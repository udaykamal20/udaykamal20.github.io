<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Uday  Kamal</title>
    <meta name="author" content="Uday  Kamal" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:ital,wght@0,300;0,400;0,600;1,300;1,600&display=swap">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/udaydp.png"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://udaykamal20.github.io/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
<header>

  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="https://udaykamal20.github.io/"><span class="font-weight-bold">Uday</span>   Kamal</a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">About</a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">Blog</a>
          </li>
          <!-- CV -->
          <li class="nav-item"><a class="nav-link" href="/assets/pdf/Uday_CV.pdf" target="_blank">CV</a></li>

          <!-- Other pages -->

          <!-- Toogle theme mode -->
          <div class="toggle-container">
            <a id="light-toggle">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </a>
          </div>
        </ul>
      </div>
    </div>
  </nav>
</header>




    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <!-- <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Uday</span>  Kamal
          </h1>
          <p class="desc"></p>
        </header> -->

        <article>
          <div class="profile float-right">
<figure>

  <picture>
     
     <source media="(max-width: 480px)" srcset="/assets/img/udaydp-480.webp"></source>
     <source media="(max-width: 800px)" srcset="/assets/img/udaydp-800.webp"></source>
     <source media="(max-width: 1400px)" srcset="/assets/img/udaydp-1400.webp"></source>
     

    <!-- Fallback to the original file -->
    <img class="img-fluid z-dept-1 rounded" src="/assets/img/udaydp.png" alt="udaydp.png">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder. -->
<p>Hi, I am Uday! I am a final year ECE PhD student at Georgia Tech, advised by <a href="https://www.ece.gatech.edu/faculty-staff-directory/saibal-mukhopadhyay" target="_blank" rel="noopener noreferrer">Prof. Dr. Saibal Mukhopadhyay</a>.</p>

<p>My research interests lie broadly in building computationally efficient, intelligent perception-capable systems. I am also interested in Robot Learning, World Models, and Embodied AI. Currently, I’m working on memory-augmented spatiotemporal representation learning with an application towards event-based perception.</p>

<p>Before starting my Ph.D., I finished my undergraduate in EEE from Bangladesh University of Engineering and Technology <a href="https://www.buet.ac.bd/web/#/" target="_blank" rel="noopener noreferrer">(BUET)</a> where I have worked on several research projects related to Computer Vision (small object detection, and segmentation), ML-assisted medical image synthesis, and analysis.</p>

<p>Feel free to reach me at: uday[dot]kamal[at]gatech.edu</p>

          </div>

          <!-- News -->          
          <div class="news">
            <h2>News</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row" width="15%">Mar 31, 2025</th>
                  <td>
                    Our work on architecture and quantization co-policy search in an end-to-end differentiable manner has been accepted in <strong>TMLR</strong>!
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" width="15%">Feb 28, 2025</th>
                  <td>
                    Our work on event-based collective dynamics learning of multi-agent systems has been accepted in <strong>L4DC</strong>!
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" width="15%">Jul 17, 2024</th>
                  <td>
                    Our work on event-based dense representation with compute efficient adaptive update got accepted in <strong>ECCV</strong>!
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" width="15%">Jun 20, 2023</th>
                  <td>
                    Our work on memory augmented spatiotemporal representation learning got accepted in <strong>ICLR (Notable-Top-25%)</strong>!
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row" width="15%">Mar 6, 2023</th>
                  <td>
                    I’ll be joining <a href="https://www.amazon.science/research-areas/robotics" target="_blank" rel="noopener noreferrer">Amazon Robotics</a> as an Applied Scientiest II intern in <del>summer</del> fall 2023!

 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          <!-- Selected papers -->
          <div class="publications">
            <h2>Selected Publications</h2>
            <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/qdarts.png"></div>

        <!-- Entry bib key -->
        <div id="anonymous2025tmlr" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">∇QDARTS: Quantization as an Elastic Dimension to Differentiable NAS</div>
          <!-- Author -->
          <div class="author">Payman Behnam*, 
                  <em>Uday Kamal*</em>, Sanjana Vijay Ganesh, Zhaoyi Li, Michael Andrew Jurado, Alind Khare, Igor Fedorov, Gaowen Liu, and  Alexey Tumanov
          <!-- Journal/Book title and date -->
          <div class="venue">
            <!-- </div>
            <div class="mention"></div> --><div class="periodical">
              <em>TMLR,</em> 2025<font color="#FF3636"><strong></strong></font>
            </div>

          </div>

        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openreview.net/pdf?id=ubrOSWyTS8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Differentiable Neural Architecture Search methods efficiently find high-accuracy architectures 
            using gradient-based optimization in a continuous domain, saving computational resources.
            Mixed-precision search helps optimize precision within a fixed architecture. However, applying
            it to a NAS-generated network doesn’t assure optimal performance as the optimized quantized
            architecture may not emerge from a standalone NAS method. In light of these considerations,
            this paper introduces ∇QDARTS, a novel approach that combines differentiable NAS with
            mixed-precision search for both weight and activation. ∇QDARTS aims to identify the optimal
            mixed-precision neural architecture capable of achieving remarkable accuracy while operating
            with minimal computational requirements in a single shot, end-to-end differentiable framework
            obviating the need for pertaining and proxy. Compared to fp32, ∇QDARTS shows impressive
            performance on CIFAR10 with (2,4) bit precision, reducing bit operations by 160× with
            a slight 1.57% accuracy drop. Increasing the capacity enables ∇QDARTS to match fp32
            accuracy while reducing bit operations by 18×. For the ImageNet dataset, with just (2,4)
            bit precision, ∇QDARTS outperforms state-of-the-art methods such as APQ, SPOS, OQA,
            and MNAS by 2.3%, 2.9%, 0.3%, and 2.7% in terms of accuracy. By incorporating (2,4,8)
            bit precision, ∇QDARTS further minimizes the accuracy drop to a 1% compared to fp32,
            alongside a substantial reduction of 17× in required bit operations and 2.6× in memory
            footprint. In terms of bit-operation (memory footprint) ∇QDARTS excels over APQ, SPOS,
            OQA, and MNAS with similar accuracy by 2.3× (12×), 2.4× (3×), 13% (6.2×), 3.4× (37%),
            for bit-operation (memory footprint), respectively. ∇QDARTS enhances the overall search
            and training efficiency, achieving a 3.1× and 1.54× improvement over APQ and OQA,
            respectively.</p>
          </div>
        </div>
      </div>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/evmap.png"></div>

        <!-- Entry bib key -->
        <div id="anonymous2025l4dc" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">Learning Collective Dynamics of Multi-Agent Systems using Event-based Vision</div>
          <!-- Author -->
          <div class="author">Minah Lee, 
                  <em>Uday Kamal</em>, and  Saibal Mukhopadhyay
          <!-- Journal/Book title and date -->
          <div class="venue">
            <!-- </div>
            <div class="mention"></div> --><div class="periodical">
              <em>L4DC,</em> 2025<font color="#FF3636"><strong></strong></font>
            </div>

          </div>

        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2411.07039" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper proposes a novel problem: vision-based perception to learn and predict the collective dynamics of multi-agent systems, specifically focusing on interaction strength and convergence time. Multi-agent systems are defined as collections of more than ten interacting agents that exhibit complex group behaviors. Unlike prior studies that assume knowledge of agent positions, we focus on deep learning models to directly predict collective dynamics from visual data, captured as frames or events. Due to the lack of relevant datasets, we create a simulated dataset using a state-of-the-art flocking simulator, coupled with a vision-to-event conversion framework. We empirically demonstrate the effectiveness of event-based representation over traditional frame-based methods in predicting these collective behaviors. Based on our analysis, we present event-based vision for Multi-Agent dynamic Prediction (evMAP), a deep learning architecture designed for real-time, accurate understanding of interaction strength and collective behavior emergence in multi-agent systems.</p>
          </div>
        </div>
      </div>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/eccv.png"></div>

        <!-- Entry bib key -->
        <div id="anonymous2024eccv" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">Efficient Learning of Event-based Dense Representation using Hierarchical Memories with Adaptive Update</div>
          <!-- Author -->
          <div class="author">
                  <em>Uday Kamal</em>, and  Saibal Mukhopadhyay
          <!-- Journal/Book title and date -->
          <div class="venue">
            <!-- </div>
            <div class="mention"></div> --><div class="periodical">
              <em>ECCV,</em> 2024<font color="#FF3636"><strong></strong></font>
            </div>

          </div>

        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10733.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Leveraging the high temporal resolution of an event-based camera requires highly efficient event-by-event processing. However, dense prediction tasks require explicit pixel-level association, which is challenging for event-based processing frameworks. Existing works aggregate the events into a static frame-like representation at the cost of a much slower processing rate and high compute cost. To address this challenge, this work introduces an event-based spatiotemporal representation learning framework for efficiently solving dense prediction tasks. We uniquely handle the sparse, asynchronous events using an unstructured, set-based approach and project them into a hierarchically organized multi-level latent memory space that preserves the pixel-level structure. Low-level event streams are dynamically encoded into these latent structures through an explicit attention-based spatial association. Unlike existing works that update these memory stacks at a fixed rate, we introduce a data-adaptive update rate that recurrently keeps track of the past memory states and learns to update the corresponding memory stacks only when it has substantial new information, thereby improving the overall compute latency. Our method consistently achieves competitive performance across different event-based dense prediction tasks while ensuring much lower latency compared to the existing methods.</p>
          </div>
        </div>
      </div>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/eventformer.svg"></div>

        <!-- Entry bib key -->
        <div id="anonymous2023associative" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception</div>
          <!-- Author -->
          <div class="author">
                  <em>Uday Kamal*</em>, Saurabh Dash*, and  Saibal Mukhopadhyay
          <!-- Journal/Book title and date -->
          <div class="venue">
            <!-- </div>
            <div class="mention"></div> --><div class="periodical">
              <em>ICLR,</em> 2023<font color="#FF3636"><strong> (notable-25%)</strong></font>
            </div>

          </div>

        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openreview.net/forum?id=ZCStthyW-TD" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We propose EventFormer, a computationally efficient event-based representation learning framework for asynchronously processing event camera data. EventFormer treats sparse input events as a spatially unordered set and models their spatial interactions using self-attention mechanism. An associative memory-augmented recurrent module is used to correlate with the stored representation computed from past events. A memory addressing mechanism is proposed to store and retrieve the latent states only where these events occur and update them only when they occur. The representation learning shift from input space to the latent memory space resulting in reduced computation cost for processing each event. We show that EventFormer achieves 0.5% and 9% better accuracy with 30000x and 200x less computation compared to the state-of-the-art dense and event-based method, respectively, on event-based object recognition datasets.</p>
          </div>
        </div>
      </div>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/anatomyx.png"></div>

        <!-- Entry bib key -->
        <div id="kamal2022anatomy" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">Anatomy-xnet: An anatomy aware convolutional neural network for thoracic disease classification in chest x-rays</div>
          <!-- Author -->
          <div class="author">
                  <em>Uday Kamal</em>, Mohammad Zunaed, Nusrat Binta Nizam, and  Taufiq Hasan
          <!-- Journal/Book title and date -->
          <div class="venue">
            <!-- </div>
            <div class="mention"></div> --><div class="periodical">
              <em>IEEE Journal of Biomedical and Health Informatics (JBHI),</em> 2022
            </div>

          </div>

        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/document/9860074" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Thoracic disease detection from chest radiographs using deep learning methods has been an active area of research in the last decade. Most previous methods attempt to focus on the diseased organs of the image by identifying spatial regions responsible for significant contributions to the model’s prediction. In contrast, expert radiologists first locate the prominent anatomical structures before determining if those regions are anomalous. Therefore, integrating anatomical knowledge within deep learning models could bring substantial improvement in automatic disease classification. Motivated by this, we propose Anatomy-XNet, an anatomy-aware attention-based thoracic disease classification network that prioritizes the spatial features guided by the pre-identified anatomy regions. We adopt a semi-supervised learning method by utilizing available small-scale organ-level annotations to locate the anatomy regions in large-scale datasets where the organ-level annotations are absent. The proposed Anatomy-XNet uses the pre-trained DenseNet-121 as the backbone network with two corresponding structured modules, the Anatomy Aware Attention (A 3 ) and Probabilistic Weighted Average Pooling, in a cohesive framework for anatomical attention learning. We experimentally show that our proposed method sets a new state-of-the-art benchmark by achieving an AUC score of 85.78%, 92.07%, and, 84.04% on three publicly available large-scale CXR datasets–NIH, Stanford CheXpert, and MIMIC-CXR, respectively. This not only proves the efficacy of utilizing the anatomy segmentation knowledge to improve the thoracic disease classification but also demonstrates the generalizability of the proposed framework.</p>
          </div>
        </div>
      </div>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/dfrtsd2.png"></div>

        <!-- Entry bib key -->
        <div id="ahmed2021dfr" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">DFR-TSD: A deep learning based framework for robust traffic sign detection under challenging weather conditions</div>
          <!-- Author -->
          <div class="author">Sabbir Ahmed, 
                  <em>Uday Kamal</em>, and  Md Kamrul Hasan
          <!-- Journal/Book title and date -->
          <div class="venue">
            <!-- </div>
            <div class="mention"></div> --><div class="periodical">
              <em>IEEE Transactions on Intelligent Transportation Systems (T-ITS),</em> 2021
            </div>

          </div>

        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9345465" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Robust traffic sign detection and recognition (TSDR) is of paramount importance for the successful realization of autonomous vehicle technology. The importance of this task has led to vast amount of research efforts and many promising methods have been proposed in the existing literature. However, most of these methods have been evaluated on clean and challenge-free datasets and overlooked the performance deterioration associated with different challenging conditions (CCs) that obscure the traffic-sign images captured in the wild. In this paper, we look at the TSDR problem under CCs and focus on the performance degradation associated with them. To this end, we propose a Convolutional Neural Network (CNN) based prior enhancement focused TSDR framework. Our modular approach consists of a CNN-based challenge classifier, Enhance-Net–an encoder-decoder CNN architecture for image enhancement, and two separate CNN architectures for sign-detection and classification. We propose a novel training pipeline for Enhance-Net that focuses on the enhancement of the traffic sign regions (instead of the whole image) in the challenging images subject to their accurate detection. We used CURE-TSD dataset consisting of traffic videos captured under different CCs to evaluate the efficacy of our approach. We experimentally show that our method obtains an overall precision and recall of 91.1% and 70.71% that is 7.58% and 35.90% improvement in precision and recall, respectively, compared to the current benchmark. Furthermore, we compare our approach with different CNN-based TSDR methods and show that our approach outperforms them by a large margin.</p>
          </div>
        </div>
      </div>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/dswenet.png"></div>

        <!-- Entry bib key -->
        <div id="ahmed2021dswe" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">DSWE-Net: A deep learning approach for shear wave elastography and lesion segmentation using single push acoustic radiation force</div>
          <!-- Author -->
          <div class="author">Shahed Ahmed, 
                  <em>Uday Kamal</em>, and  Md Kamrul Hasan
          <!-- Journal/Book title and date -->
          <div class="venue">
            <!-- </div>
            <div class="mention"></div> --><div class="periodical">
              <em>Ultrasonics,</em> 2021
            </div>

          </div>

        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://www.sciencedirect.com/science/article/pii/S0041624X20302213" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Ultrasound-based non-invasive elasticity imaging modalities have received significant consideration for tissue characterization over the last few years. Though substantial advances have been made, the conventional Shear Wave Elastography (SWE) methods still suffer from poor image quality in regions far from the push location, particularly those which rely on single focused ultrasound push beam to generate shear waves. In this study, we propose DSWE-Net, a novel deep learning-based approach that is able to construct Young’s modulus maps from ultrasonically tracked tissue velocity data resulting from a single acoustic radiation force (ARF) push. The proposed network employs a 3D convolutional encoder, followed by a recurrent block consisting of several Convolutional Long Short-Term Memory (ConvLSTM) layers to extract high-level spatio-temporal features from different time-frames of the input velocity data. Finally, a pair of coupled 2D convolutional decoder blocks reconstructs the modulus image and additionally performs inclusion segmentation by generating a binary mask. We also propose a multi-task learning loss function for end-to-end training of the network with 1260 data samples obtained from a simulation environment which include both bi-level and multi-level phantom structures. The performance of the proposed network is evaluated on 140 synthetic test data and the results are compared both qualitatively and quantitatively with that of the current state of the art method, Local Phase Velocity Based Imaging (LPVI). With an average SSIM of 0.90, RMSE of 0.10 and 20.69 dB PSNR, DSWE-Net performs much better on the imaging task compared to LPVI. Our method also achieves an average IoU score of 0.81 for the segmentation task which makes it suitable for localizing inclusions as well. In this initial study, we also show that our method gains an overall improvement of 0.09 in SSIM, 4.81 dB in PSNR, 2.02 dB in CNR, and 0.09 in RMSE over LPVI on a completely unseen set of CIRS tissue mimicking phantom data. This proves its better generalization capability and shows its potential for use in real-world clinical practice.</p>
          </div>
        </div>
      </div>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/miccai.png"></div>

        <!-- Entry bib key -->
        <div id="kamal2020lung" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">Lung cancer tumor region segmentation using recurrent 3d-denseunet</div>
          <!-- Author -->
          <div class="author">
                  <em>Uday Kamal</em>, Abdul Muntakim Rafi, Rakibul Hoque, Jonathan Wu, and  Md Kamrul Hasan
          <!-- Journal/Book title and date -->
          <div class="venue">
            <!-- </div>
            <div class="mention"></div> --><div class="periodical">
              <em>MICCAI Workshop, </em> 2020
            </div>

          </div>

        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9345465" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The performance of a computer-aided automated diagnosis system of lung cancer from Computed Tomography (CT) volumetric images greatly depends on the accurate detection and segmentation of tumor regions. In this paper, we present Recurrent 3D-DenseUNet, a novel deep learning based architecture for volumetric lung tumor segmentation from CT scans. The proposed architecture consists of a 3D encoder block that learns to extract fine-grained spatial and coarse-grained temporal features, a recurrent block of multiple Convolutional Long Short-Term Memory (ConvLSTM) layers to extract fine-grained spatio-temporal information, and finally a 3D decoder block to reconstruct the desired volume segmentation masks from the latent feature space. The encoder and decoder blocks consist of several 3D-convolutional layers that are densely connected among themselves so that necessary feature aggregation can occur throughout the network. During prediction, we apply selective thresholding followed by morphological operation, on top of the network prediction, to better differentiate between tumorous and non-tumorous image-slices, which shows more promise than only thresholding-based approaches. We train and test our network on the NSCLC-Radiomics dataset of 300 patients, provided by The Cancer Imaging Archive (TCIA) for the 2018 IEEE VIP Cup. Moreover, we perform an extensive ablation study of different loss functions in practice for this task. The proposed network outperforms other state-of-the-art 3D segmentation architectures with an average dice score of 0.7228.</p>
          </div>
        </div>
      </div>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/cvprw.png"></div>

        <!-- Entry bib key -->
        <div id="rafi2019application" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">Application of DenseNet in Camera Model Identification and Post-processing Detection.</div>
          <!-- Author -->
          <div class="author">Abdul Muntakim Rafi, 
                  <em>Uday Kamal</em>, Rakibul Hoque, Abid Abrar, Sowmitra Das, Robert Laganiere, Md Kamrul Hasan, and   others
          <!-- Journal/Book title and date -->
          <div class="venue">
            <!-- </div>
            <div class="mention"></div> --><div class="periodical">
              <em>CVPR workshop, </em> 2019
            </div>

          </div>

        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Media%20Forensics/Rafi_Application_of_DenseNet_in_Camera_Model_Identification_and_Post-processing_Detection_CVPRW_2019_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Camera model identification has earned paramount importance in the field of image forensics with an upsurge of digitally altered images which are constantly being shared through websites, media, and social applications. But, the task of identification becomes quite challenging if metadata are absent from the image and/or if the image has been post-processed. In this paper, we present a DenseNet pipeline to solve the problem of identifying the source camera-model of an image. Our approach is to extract patches of 256 x 256 from a labeled image dataset and apply augmentations, i.e., Empirical Mode Decomposition (EMD). We use this extended dataset to train a Neural Network with the DenseNet-201 architecture. We concatenate the output features for 3 different sizes (64x64, 128x128, 256x256) and pass them to a secondary network to make the final prediction. This strategy proves to be very robust for identifying the source camera model, even when the original image is post-processed. Our model has been trained and tested on the Forensic Camera-Model Identification Dataset provided for the IEEE Signal Processing (SP) Cup 2018. During testing we achieved an overall accuracy of 98.37%, which is the current state-of-the-art on this dataset using a single model. We used transfer learning and tested our model on the Dresden Database for Camera Model Identification, with an overall test accuracy of over 99% for 19 models. In addition, we demonstrate that the proposed pipeline is suit- able for other image-forensic classification tasks, such as, detecting the type of post-processing applied to an image with an accuracy of 96.66% - which indicates the generality of our approach.</p>
          </div>
        </div>
      </div>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-4 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/tits.png"></div>

        <!-- Entry bib key -->
        <div id="kamal2019automatic" class="col-sm-9">
        
          <!-- Title -->
          <div class="title">Automatic traffic sign detection and recognition using SegU-Net and a modified Tversky loss function with L1-constraint</div>
          <!-- Author -->
          <div class="author">
                  <em>Uday Kamal</em>, Thamidul Islam Tonmoy, Sowmitra Das, and  Md Kamrul Hasan
          <!-- Journal/Book title and date -->
          <div class="venue">
            <!-- </div>
            <div class="mention"></div> --><div class="periodical">
              <em>IEEE Transactions on Intelligent Transportation Systems (T-ITS),</em> 2019
            </div>

          </div>

        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/8700606" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Traffic sign detection is a central part of autonomous vehicle technology. Recent advances in deep learning algorithms have motivated researchers to use neural networks to perform this task. In this paper, we look at traffic sign detection as an image segmentation problem and propose a deep convolutional neural network-based approach to solve it. To this end, we propose a new network, the SegU-Net, which we form by merging the state-of-the-art segmentation architectures-SegNet and U-Net to detect traffic signs from video sequences. For training the network, we use the Tversky loss function constrained by an L1 term instead of the intersection over union loss traditionally used to train segmentation networks. We use a separate network, inspired by the VGG-16 architecture, to classify the detected signs. The networks are trained on the challenge free sequences of the CURE-TSD dataset. Our proposed network outperforms the state-of-the-art object detection networks, such as the Faster R-CNN inception Resnet V2 and R-FCN Resnet 101, by a large margin and obtains a precision and recall of 94.60% and 80.21%, respectively, which is the current state of the art on this part of the dataset. In addition, the network is tested on the German Traffic Sign Detection Benchmark (GTSDB) dataset, where it achieves a precision and recall of 95.29% and 89.01%, respectively. This is on a par with the performance of the aforementioned object detection networks. These results prove the generalizability of the proposed architecture and its suitability for robust traffic sign detection in autonomous vehicles.</p>
          </div>
        </div>
      </div>

          </div>

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%75%64%61%79.%6B%61%6D%61%6C@%67%61%74%65%63%68.%65%64%75" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=8AUUqTAAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/udaykamal20" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/udaykamal" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/udday2014" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            
            </div>

            <div class="contact-note">
              
            </div>
            
          </div>
        

</li>
</div>

    </li>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Uday  Kamal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </li>
</div>
</li>
</div>
</li>
</div>
</li>
</div>
</li>
</div>
</li>
</div>
</li>
</div>
</li></ol>
</div></article>
</div>
</div></body>
</html>

